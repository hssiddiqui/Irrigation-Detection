{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Convert checkpointed model from GCP VM to Keras saved model for deployment in\n",
        "irrigation_detection_inference.ipynb"
      ],
      "metadata": {
        "id": "rMnexEACU4cb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSgR-IYpUk5j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87a6cd7c-5302-490a-b52b-bb6f1407988f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n",
            "Loading existing checkpoint: 20221226-184032/epoch_19_top_min_f1_0.9150-8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 56). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "transformer_trained_2020/variables/variables.index\n",
            "transformer_trained_2020/variables/variables.data-00000-of-00001\n",
            "transformer_trained_2020/saved_model.pb\n",
            "transformer_trained_2020/keras_metadata.pb\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.python import saved_model\n",
        "def convert_tf_model(input_model_folder, output_model_folder):\n",
        "\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
        "    from tensorflow.keras import Model, Sequential\n",
        "    from tensorflow.keras import layers\n",
        "    from zipfile import ZipFile\n",
        "\n",
        "    # Define model encoder\n",
        "    def _transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "        # Normalization and Attention\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
        "        x = layers.MultiHeadAttention(\n",
        "            key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "        )(x, x)\n",
        "        x = layers.Dropout(dropout)(x)\n",
        "        res = x + inputs\n",
        "\n",
        "        # Feed Forward Part\n",
        "        x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
        "        x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(dropout)(x)\n",
        "        x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "        return x + res\n",
        "\n",
        "    # Build model\n",
        "    def _build_model(\n",
        "        input_shape,\n",
        "        head_size,\n",
        "        num_heads,\n",
        "        ff_dim,\n",
        "        num_transformer_blocks,\n",
        "        mlp_units,\n",
        "        dropout=0,\n",
        "        mlp_dropout=0,\n",
        "    ):\n",
        "        inputs = tf.keras.Input(shape=input_shape)\n",
        "        x = inputs\n",
        "        for _ in range(num_transformer_blocks):\n",
        "            x = _transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "        x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "        for dim in mlp_units:\n",
        "            x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "            x = layers.Dropout(mlp_dropout)(x)\n",
        "\n",
        "        n_classes = 2\n",
        "        outputs = layers.Dense(n_classes, activation='softmax')(x)\n",
        "\n",
        "        return Model(inputs, outputs)\n",
        "\n",
        "    # Return model\n",
        "    def _return_model():\n",
        "\n",
        "        input_shape = (36,1)\n",
        "\n",
        "        model = _build_model(\n",
        "                        input_shape,\n",
        "                        head_size=64,\n",
        "                        num_heads=4,\n",
        "                        ff_dim=4,\n",
        "                        num_transformer_blocks=4,\n",
        "                        mlp_units=[32],\n",
        "                        mlp_dropout=0.4,\n",
        "                        dropout=0.25,\n",
        "                        )\n",
        "\n",
        "        lr = 1e-4\n",
        "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=lr)\n",
        "\n",
        "\n",
        "        with ZipFile(input_model_folder, 'r') as zip:\n",
        "            zip.extractall()\n",
        "            print('Done')\n",
        "\n",
        "        prev_checkpoint_prefix = input_model_folder.replace('.zip', '')\n",
        "        checkpoint = tf.train.Checkpoint(\n",
        "            optimizer=optimizer,\n",
        "            model=model,\n",
        "        )\n",
        "        print(f'Loading existing checkpoint: {tf.train.latest_checkpoint(prev_checkpoint_prefix)}')\n",
        "        status = checkpoint.restore(tf.train.latest_checkpoint(prev_checkpoint_prefix)).expect_partial()\n",
        "        # print(status.assert_existing_objects_matched())\n",
        "\n",
        "        model.compile(optimizer)\n",
        "\n",
        "        return model\n",
        "\n",
        "    # Save model locally\n",
        "    def _save_model(model):\n",
        "        model.save(output_model_folder)\n",
        "\n",
        "    # Upload converted model to GCP\n",
        "    def _upload_files():\n",
        "\n",
        "        from google.cloud import storage\n",
        "        from google.colab import auth\n",
        "        from glob import glob\n",
        "        import os\n",
        "        # Authenticate Google account\n",
        "        auth.authenticate_user()\n",
        "\n",
        "        bucket_name = 'gee_irrigation_detection'\n",
        "        storage_client = storage.Client()\n",
        "        bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "        \"\"\"Upload files to GCP bucket.\"\"\"\n",
        "        files = glob(output_model_folder + \"/**\", recursive=True)\n",
        "        for file in files:\n",
        "            if os.path.isfile(file):\n",
        "                print(file)\n",
        "                blob = bucket.blob(f'saved_models/{file}')\n",
        "                blob.upload_from_filename(file)\n",
        "\n",
        "        return f'Uploaded to \"{bucket_name}\" bucket.'\n",
        "\n",
        "\n",
        "\n",
        "    # Execute function\n",
        "    model = _return_model()\n",
        "    _save_model(model)\n",
        "    _upload_files()\n",
        "\n",
        "\n",
        "\n",
        "# Upload folder containing Transformer model checkpoint, data, and index files\n",
        "# Folder will be a zip file with a datetime as a name\n",
        "input_model_folder = '20221226-184032.zip'\n",
        "\n",
        "# Define out folder\n",
        "output_model_folder = 'transformer_trained_2020'\n",
        "\n",
        "convert_tf_model(input_model_folder, output_model_folder)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_annual_models():\n",
        "\n",
        "    from google.cloud import storage\n",
        "    from google.colab import auth\n",
        "    import numpy as np\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    !pip install rasterio\n",
        "    import rasterio\n",
        "\n",
        "    # Authenticate Google account\n",
        "    auth.authenticate_user()\n",
        "\n",
        "    model_0 = 'transformer_trained_2020'\n",
        "    model_1 = 'transformer_trained_2021'\n",
        "\n",
        "    # Initialize client and find folders in bucket\n",
        "    storage_client = storage.Client()\n",
        "    bucket_name = 'gee_irrigation_detection'\n",
        "    blobs = storage_client.list_blobs(bucket_name, prefix=f'model_predictions')\n",
        "\n",
        "    # Find all images\n",
        "    pred_images = [blob.name for blob in blobs if '.tif' in blob.name]\n",
        "\n",
        "    # Find predictions by model\n",
        "    model_1_predictions = sorted([i for i in pred_images if model_0 in i])\n",
        "    model_2_predictions = sorted([i for i in pred_images if model_1 in i])\n",
        "\n",
        "    misaligned_count = 0\n",
        "    total_count = 0\n",
        "\n",
        "    # Compare predictions across models\n",
        "    for ix, image in enumerate(tqdm(model_1_predictions)):\n",
        "\n",
        "        img_0 = rasterio.open(f'gs://{bucket_name}/{image}').read()\n",
        "        img_1 = rasterio.open(f'gs://{bucket_name}/{model_2_predictions[ix]}').read()\n",
        "\n",
        "        print(img_0 - img_1)\n",
        "\n",
        "        misaligned_count += np.count_nonzero(img_0 - img_1)\n",
        "        total_count += img_0.shape[1] * img_0.shape[2]\n",
        "\n",
        "    print(f'Total misaligned: {np.round(misaligned_count/total_count, decimals=3)} ({misaligned_count}/{total_count})')\n",
        "\n",
        "compare_annual_models()"
      ],
      "metadata": {
        "id": "-ES_8W3XYmFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_x-saGU91IDB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}